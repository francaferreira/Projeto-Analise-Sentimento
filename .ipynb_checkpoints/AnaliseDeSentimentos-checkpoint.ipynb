{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd354fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0b63f9-85e3-4b57-8d63-1c722d9429f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49459, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           id                                            text_en  \\\n",
       "0          1  Once again Mr. Costner has dragged out a movie...   \n",
       "1          2  This is an example of why the majority of acti...   \n",
       "2          3  First of all I hate those moronic rappers, who...   \n",
       "3          4  Not even the Beatles could write songs everyon...   \n",
       "4          5  Brass pictures movies is not a fitting word fo...   \n",
       "...      ...                                                ...   \n",
       "49454  49456  Seeing as the vote average was pretty low, and...   \n",
       "49455  49457  The plot had some wretched, unbelievable twist...   \n",
       "49456  49458  I am amazed at how this movieand most others h...   \n",
       "49457  49459  A Christmas Together actually came before my t...   \n",
       "49458  49460  Working-class romantic drama from director Mar...   \n",
       "\n",
       "                                                 text_pt sentiment  \n",
       "0      Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
       "1      Este √© um exemplo do motivo pelo qual a maiori...       neg  \n",
       "2      Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
       "3      Nem mesmo os Beatles puderam escrever m√∫sicas ...       neg  \n",
       "4      Filmes de fotos de lat√£o n√£o √© uma palavra apr...       neg  \n",
       "...                                                  ...       ...  \n",
       "49454  Como a m√©dia de votos era muito baixa, e o fat...       pos  \n",
       "49455  O enredo teve algumas reviravoltas infelizes e...       pos  \n",
       "49456  Estou espantado com a forma como este filme e ...       pos  \n",
       "49457  A Christmas Together realmente veio antes do m...       pos  \n",
       "49458  O drama rom√¢ntico da classe trabalhadora do di...       pos  \n",
       "\n",
       "[49459 rows x 4 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('imdb-reviews-pt-br.csv')\n",
    "print(data.shape)\n",
    "data.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e023f5-fe7f-44c0-9319-2dc4e3ffa306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49459 entries, 0 to 49458\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         49459 non-null  int64 \n",
      " 1   text_en    49459 non-null  object\n",
      " 2   text_pt    49459 non-null  object\n",
      " 3   sentiment  49459 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3ac8e9-78bd-4bf0-a155-6f696f66b33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neg    24765\n",
       "pos    24694\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0eb1c6-c429-4a7a-896b-50d2857e5d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text_en', 'text_pt', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8a1065-3e45-4f04-be00-c796896a88e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mais uma vez, o Sr. Costner arrumou um filme por muito mais tempo do que o necess√°rio. Al√©m das terr√≠veis seq√º√™ncias de resgate no mar, das quais h√° muito poucas, eu simplesmente n√£o me importei com nenhum dos personagens. A maioria de n√≥s tem fantasmas no arm√°rio, e o personagem Costers √© realizado logo no in√≠cio, e depois esquecido at√© muito mais tarde, quando eu n√£o me importava. O personagem com o qual dever√≠amos nos importar √© muito arrogante e superconfiante, Ashton Kutcher. O problema √© que ele sai como um garoto que pensa que √© melhor do que qualquer outra pessoa ao seu redor e n√£o mostra sinais de um arm√°rio desordenado. Seu √∫nico obst√°culo parece estar vencendo Costner. Finalmente, quando estamos bem al√©m do meio do caminho, Costner nos conta sobre os fantasmas dos Kutchers. Somos informados de por que Kutcher √© levado a ser o melhor sem pressentimentos ou press√°gios anteriores. Nenhuma m√°gica aqui, era tudo que eu podia fazer para n√£o desligar uma hora.\n"
     ]
    }
   ],
   "source": [
    "print(data['text_pt'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7ad76-a32d-463b-8011-b1c54173b046",
   "metadata": {},
   "source": [
    "1. Remove HTML tags\n",
    "\n",
    "Regex rule:'<.*?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9508a87a-23cc-47dc-8692-fb5e5226509e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mais uma vez, o Sr. Costner arrumou um filme por muito mais tempo do que o necess√°rio. Al√©m das terr√≠veis seq√º√™ncias de resgate no mar, das quais h√° muito poucas, eu simplesmente n√£o me importei com nenhum dos personagens. A maioria de n√≥s tem fantasmas no arm√°rio, e o personagem Costers √© realizado logo no in√≠cio, e depois esquecido at√© muito mais tarde, quando eu n√£o me importava. O personagem com o qual dever√≠amos nos importar √© muito arrogante e superconfiante, Ashton Kutcher. O problema √© que ele sai como um garoto que pensa que √© melhor do que qualquer outra pessoa ao seu redor e n√£o mostra sinais de um arm√°rio desordenado. Seu √∫nico obst√°culo parece estar vencendo Costner. Finalmente, quando estamos bem al√©m do meio do caminho, Costner nos conta sobre os fantasmas dos Kutchers. Somos informados de por que Kutcher √© levado a ser o melhor sem pressentimentos ou press√°gios anteriores. Nenhuma m√°gica aqui, era tudo que eu podia fazer para n√£o desligar uma hora.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    cleaned = re.compile(r'<.*?>')\n",
    "    return re.sub(cleaned,'',text)\n",
    "    \n",
    "data.text_pt = data.text_pt.apply(clean)\n",
    "data.text_pt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67b35f-b5e8-4c1c-90c8-6e2a67d0b34c",
   "metadata": {},
   "source": [
    "2. Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d544a49c-7f6f-4c4f-8b72-35c42626f92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mais uma vez  o Sr  Costner arrumou um filme por muito mais tempo do que o necess√°rio  Al√©m das terr√≠veis seq√º√™ncias de resgate no mar  das quais h√° muito poucas  eu simplesmente n√£o me importei com nenhum dos personagens  A maioria de n√≥s tem fantasmas no arm√°rio  e o personagem Costers √© realizado logo no in√≠cio  e depois esquecido at√© muito mais tarde  quando eu n√£o me importava  O personagem com o qual dever√≠amos nos importar √© muito arrogante e superconfiante  Ashton Kutcher  O problema √© que ele sai como um garoto que pensa que √© melhor do que qualquer outra pessoa ao seu redor e n√£o mostra sinais de um arm√°rio desordenado  Seu √∫nico obst√°culo parece estar vencendo Costner  Finalmente  quando estamos bem al√©m do meio do caminho  Costner nos conta sobre os fantasmas dos Kutchers  Somos informados de por que Kutcher √© levado a ser o melhor sem pressentimentos ou press√°gios anteriores  Nenhuma m√°gica aqui  era tudo que eu podia fazer para n√£o desligar uma hora '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_special(text):\n",
    "    rem = ''\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            rem = rem + i\n",
    "        else:\n",
    "            rem = rem + ' '\n",
    "    return rem\n",
    "\n",
    "data.text_pt = data.text_pt.apply(is_special)\n",
    "data.text_pt[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ffa42-96f0-4ee1-b4a1-45cc6a49a1ef",
   "metadata": {},
   "source": [
    "3. Convert everything to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a1b382c-092f-490f-b9c4-f7d4e6fdca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mais uma vez  o sr  costner arrumou um filme por muito mais tempo do que o necess√°rio  al√©m das terr√≠veis seq√º√™ncias de resgate no mar  das quais h√° muito poucas  eu simplesmente n√£o me importei com nenhum dos personagens  a maioria de n√≥s tem fantasmas no arm√°rio  e o personagem costers √© realizado logo no in√≠cio  e depois esquecido at√© muito mais tarde  quando eu n√£o me importava  o personagem com o qual dever√≠amos nos importar √© muito arrogante e superconfiante  ashton kutcher  o problema √© que ele sai como um garoto que pensa que √© melhor do que qualquer outra pessoa ao seu redor e n√£o mostra sinais de um arm√°rio desordenado  seu √∫nico obst√°culo parece estar vencendo costner  finalmente  quando estamos bem al√©m do meio do caminho  costner nos conta sobre os fantasmas dos kutchers  somos informados de por que kutcher √© levado a ser o melhor sem pressentimentos ou press√°gios anteriores  nenhuma m√°gica aqui  era tudo que eu podia fazer para n√£o desligar uma hora \n"
     ]
    }
   ],
   "source": [
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "data['text_pt'] = data['text_pt'].apply(to_lower)\n",
    "print(data['text_pt'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1ac4e-c292-4fd6-9c96-fe8285c1320a",
   "metadata": {},
   "source": [
    "4. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f87552-1494-4fd2-a716-33d3c754a365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jef\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jef\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mais uma vez  o sr  costner arrumou um filme por muito mais tempo do que o necess√°rio  al√©m das terr√≠veis seq√º√™ncias de resgate no mar  das quais h√° muito poucas  eu simplesmente n√£o me importei com nenhum dos personagens  a maioria de n√≥s tem fantasmas no arm√°rio  e o personagem costers √© realizado logo no in√≠cio  e depois esquecido at√© muito mais tarde  quando eu n√£o me importava  o personagem com o qual dever√≠amos nos importar √© muito arrogante e superconfiante  ashton kutcher  o problema √© que ele sai como um garoto que pensa que √© melhor do que qualquer outra pessoa ao seu redor e n√£o mostra sinais de um arm√°rio desordenado  seu √∫nico obst√°culo parece estar vencendo costner  finalmente  quando estamos bem al√©m do meio do caminho  costner nos conta sobre os fantasmas dos kutchers  somos informados de por que kutcher √© levado a ser o melhor sem pressentimentos ou press√°gios anteriores  nenhuma m√°gica aqui  era tudo que eu podia fazer para n√£o desligar uma hora \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def rem_stopwords(text):\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    words = word_tokenize(text)\n",
    "    return [w for w in words if w not in stop_words]\n",
    "\n",
    "data['text_pt'] = data['text_pt'].apply(to_lower)\n",
    "print(data['text_pt'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3eaad-3342-4cfd-8068-601361635087",
   "metadata": {},
   "source": [
    "5. Stem the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d12e8e60-eeb1-45c7-b210-6e9c1a989de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m a i s   u m a   v e z     o   s r     c o s t n e r   a r r u m o u   u m   f i l m e   p o r   m u i t o   m a i s   t e m p o   d o   q u e   o   n e c e s s √° r i o     a l √© m   d a s   t e r r √≠ v e i s   s e q √º √™ n c i a s   d e   r e s g a t e   n o   m a r     d a s   q u a i s   h √°   m u i t o   p o u c a s     e u   s i m p l e s m e n t e   n √£ o   m e   i m p o r t e i   c o m   n e n h u m   d o s   p e r s o n a g e n s     a   m a i o r i a   d e   n √≥ s   t e m   f a n t a s m a s   n o   a r m √° r i o     e   o   p e r s o n a g e m   c o s t e r s   √©   r e a l i z a d o   l o g o   n o   i n √≠ c i o     e   d e p o i s   e s q u e c i d o   a t √©   m u i t o   m a i s   t a r d e     q u a n d o   e u   n √£ o   m e   i m p o r t a v a     o   p e r s o n a g e m   c o m   o   q u a l   d e v e r √≠ a m o s   n o s   i m p o r t a r   √©   m u i t o   a r r o g a n t e   e   s u p e r c o n f i a n t e     a s h t o n   k u t c h e r     o   p r o b l e m a   √©   q u e   e l e   s a i   c o m o   u m   g a r o t o   q u e   p e n s a   q u e   √©   m e l h o r   d o   q u e   q u a l q u e r   o u t r a   p e s s o a   a o   s e u   r e d o r   e   n √£ o   m o s t r a   s i n a i s   d e   u m   a r m √° r i o   d e s o r d e n a d o     s e u   √∫ n i c o   o b s t √° c u l o   p a r e c e   e s t a r   v e n c e n d o   c o s t n e r     f i n a l m e n t e     q u a n d o   e s t a m o s   b e m   a l √© m   d o   m e i o   d o   c a m i n h o     c o s t n e r   n o s   c o n t a   s o b r e   o s   f a n t a s m a s   d o s   k u t c h e r s     s o m o s   i n f o r m a d o s   d e   p o r   q u e   k u t c h e r   √©   l e v a d o   a   s e r   o   m e l h o r   s e m   p r e s s e n t i m e n t o s   o u   p r e s s √° g i o s   a n t e r i o r e s     n e n h u m a   m √° g i c a   a q u i     e r a   t u d o   q u e   e u   p o d i a   f a z e r   p a r a   n √£ o   d e s l i g a r   u m a   h o r a  '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_txt(text):\n",
    "    ss = SnowballStemmer('portuguese')\n",
    "    return \" \".join([ss.stem(w) for w in text])\n",
    "\n",
    "data.text_pt = data.text_pt.apply(stem_txt)\n",
    "data.text_pt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3da07-30e7-4dec-bd35-c6066c0b0351",
   "metadata": {},
   "source": [
    "Creating the Model\n",
    "\n",
    "1. Creating Bag of words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fa5a1c4-4a30-4f06-a0f3-459e11e0e3eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m      3\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mfit_transform(data\u001b[38;5;241m.\u001b[39mtext_pt)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape = \u001b[39m\u001b[38;5;124m\"\u001b[39m,X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my.shape = \u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         )\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "X = np.array(data.iloc[:,0].values)\n",
    "y = np.array(data.sentiment.values)\n",
    "cv = CountVectorizer(max_features = 1000)\n",
    "X = cv.fit_transform(data.text_pt).toarray()\n",
    "print(\"X.shape = \",X.shape)\n",
    "print(\"y.shape = \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b65a0b4-1acb-4dd5-818c-a42753e6834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preenche valores faltantes com string vazia e garante o tipo string\n",
    "data['text_pt'] = data['text_pt'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91cfd8e-2a94-4e8c-ac3d-7915b366f84f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# O erro original provavelmente vinha daqui, pois o CountVectorizer \u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# pode estar usando a lista padr√£o (ingl√™s) ou None, mas o NLTK tem a lista correta.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, stop_words \u001b[38;5;241m=\u001b[39m pt_stopwords)\n\u001b[1;32m---> 19\u001b[0m X \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mfit_transform(data\u001b[38;5;241m.\u001b[39mtext_pt)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape = \u001b[39m\u001b[38;5;124m\"\u001b[39m,X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my.shape = \u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         )\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Importar o NLTK e a lista de stop words em Portugu√™s\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Certifique-se de que voc√™ baixou a lista de stop words do NLTK.\n",
    "# Se for a primeira vez, execute: nltk.download('stopwords')\n",
    "\n",
    "# 2. Obter a lista de stop words em Portugu√™s\n",
    "pt_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "# 3. Aplicar o CountVectorizer\n",
    "y = np.array(data.sentiment.values)\n",
    "\n",
    "# O erro original provavelmente vinha daqui, pois o CountVectorizer \n",
    "# pode estar usando a lista padr√£o (ingl√™s) ou None, mas o NLTK tem a lista correta.\n",
    "cv = CountVectorizer(max_features = 1000, stop_words = pt_stopwords)\n",
    "X = cv.fit_transform(data.text_pt).toarray()\n",
    "\n",
    "print(\"X.shape = \",X.shape)\n",
    "print(\"y.shape = \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed4ae59-4574-4b2e-9f1e-2e11f6badff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Execute apenas a primeira vez que usar o NLTK\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Baixando dados de stopwords do NLTK...\")\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41ab1012-8af5-4f58-8d2b-bf5e476f91aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro persistente: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "Tentando a vetoriza√ß√£o SEM Stop Words como alternativa...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     X \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mfit_transform(data\u001b[38;5;241m.\u001b[39mtext_pt)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSucesso na vetoriza√ß√£o!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         )\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Tente sem nenhuma filtragem de stop words como alternativa\u001b[39;00m\n\u001b[0;32m     35\u001b[0m cv_sem_stopwords \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;66;03m# stop_words=None √© o padr√£o\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m X \u001b[38;5;241m=\u001b[39m cv_sem_stopwords\u001b[38;5;241m.\u001b[39mfit_transform(data\u001b[38;5;241m.\u001b[39mtext_pt)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSucesso SEM Stop Words!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape = \u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         )\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd # Assumindo que 'data' √© um DataFrame Pandas\n",
    "\n",
    "# --- Passo 1: Limpeza e Download do NLTK (Garantindo que a lista exista) ---\n",
    "# Se for a primeira vez, execute: nltk.download('stopwords')\n",
    "# (Ou use o bloco 'try/except' acima para um download seguro)\n",
    "\n",
    "# Limpeza dos dados: Garante que todos s√£o strings\n",
    "data['text_pt'] = data['text_pt'].fillna('').astype(str)\n",
    "\n",
    "# --- Passo 2: Configura√ß√£o da Vetoriza√ß√£o ---\n",
    "y = np.array(data.sentiment.values)\n",
    "\n",
    "# 1. Obter a lista de stop words em Portugu√™s\n",
    "pt_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "# 2. Inicializar o CountVectorizer\n",
    "# max_features = 1000\n",
    "cv = CountVectorizer(max_features = 1000, stop_words = pt_stopwords)\n",
    "\n",
    "# --- Passo 3: Execu√ß√£o (onde o erro ocorria) ---\n",
    "try:\n",
    "    X = cv.fit_transform(data.text_pt).toarray()\n",
    "    print(\"Sucesso na vetoriza√ß√£o!\")\n",
    "    print(\"X.shape = \", X.shape)\n",
    "    print(\"y.shape = \", y.shape)\n",
    "except ValueError as e:\n",
    "    # Se o erro persistir, provavelmente o texto √© muito curto/ru√≠do\n",
    "    print(f\"Erro persistente: {e}\")\n",
    "    print(\"\\nTentando a vetoriza√ß√£o SEM Stop Words como alternativa...\")\n",
    "    \n",
    "    # Tente sem nenhuma filtragem de stop words como alternativa\n",
    "    cv_sem_stopwords = CountVectorizer(max_features = 1000) # stop_words=None √© o padr√£o\n",
    "    X = cv_sem_stopwords.fit_transform(data.text_pt).toarray()\n",
    "    print(\"Sucesso SEM Stop Words!\")\n",
    "    print(\"X.shape = \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a70e9ae-6aa8-4402-bd1b-0891cad829cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de Strings Vazias: 0\n",
      "M√©dia de Comprimento dos Textos: 2562.6937261165813\n",
      "\n",
      "Primeiras 10 Amostras da Coluna:\n",
      "0    m a i s   u m a   v e z     o   s r     c o s ...\n",
      "1    e s t e   √©   u m   e x e m p l o   d o   m o ...\n",
      "2    p r i m e i r o   d e   t u d o   e u   o d e ...\n",
      "3    n e m   m e s m o   o s   b e a t l e s   p u ...\n",
      "4    f i l m e s   d e   f o t o s   d e   l a t √£ ...\n",
      "5    u m a   c o i s a   e n g r a c a d a   a c o ...\n",
      "6    e s t e   f i l m e   d e   t e r r o r   a l ...\n",
      "7    s e n d o   u m   f √£   d e   l o n g a   d a ...\n",
      "8      t o k y o   e y e s     f a l a   d e   u m ...\n",
      "9    f a z e n d e i r o s   r i c o s   e m   b u ...\n",
      "Name: text_pt, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1. Checar quantos valores vazios existem (depois do fillna)\n",
    "print(\"Contagem de Strings Vazias:\", (data.text_pt == '').sum())\n",
    "\n",
    "# 2. Checar a m√©dia de caracteres por documento para ver se s√£o muito curtos\n",
    "print(\"M√©dia de Comprimento dos Textos:\", data.text_pt.str.len().mean())\n",
    "\n",
    "# 3. Mostrar algumas amostras dos textos que est√£o na coluna\n",
    "print(\"\\nPrimeiras 10 Amostras da Coluna:\")\n",
    "print(data.text_pt.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "332656e0-97a8-42f7-89f5-60b19f959819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sucesso com Padr√£o Relaxado!\n",
      "X_relaxed.shape =  (49459, 77)\n"
     ]
    }
   ],
   "source": [
    "# Novo padr√£o: r'\\b\\w+\\b' captura palavras de UMA ou mais letras.\n",
    "new_token_pattern = r'\\b\\w+\\b'\n",
    "\n",
    "# Tentativa Final SEM Stop Words, mas com o padr√£o de tokeniza√ß√£o relaxado\n",
    "cv_relaxed = CountVectorizer(max_features = 1000, token_pattern = new_token_pattern)\n",
    "\n",
    "try:\n",
    "    X_relaxed = cv_relaxed.fit_transform(data.text_pt).toarray()\n",
    "    print(\"\\nSucesso com Padr√£o Relaxado!\")\n",
    "    print(\"X_relaxed.shape = \", X_relaxed.shape)\n",
    "except ValueError as e:\n",
    "    # Se o erro ocorrer aqui, seus dados s√£o APENAS ru√≠do (pontua√ß√£o/emojis)\n",
    "    print(\"\\nERRO CR√çTICO: Seus textos n√£o cont√™m letras v√°lidas ap√≥s a limpeza.\")\n",
    "    print(\"Verifique se h√° pr√©-processamento que removeu todas as letras.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8ba6010-fb92-4c7f-8077-c827e4830212",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, stop_words \u001b[38;5;241m=\u001b[39m pt_stopwords) \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Use a coluna LIMPA\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m X_final \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mfit_transform(data\u001b[38;5;241m.\u001b[39mtext_pt_cleaned)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Ap√≥s Limpeza dos Espa√ßos ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_final.shape (Esperado > 77): \u001b[39m\u001b[38;5;124m\"\u001b[39m, X_final\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         )\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Fun√ß√£o para remover espa√ßos excessivos e reformar as palavras\n",
    "def clean_excess_spaces(text):\n",
    "    # Substitui 2 ou mais espa√ßos seguidos por UM √∫nico espa√ßo\n",
    "    # (Isso vai transformar \"m a i s\" em \"m a i s\")\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # Remove espa√ßos √† esquerda e √† direita\n",
    "    return text.strip()\n",
    "\n",
    "# 1. Aplicar a limpeza\n",
    "data['text_pt_cleaned'] = data['text_pt'].apply(clean_excess_spaces)\n",
    "\n",
    "# 2. Re-rodar o CountVectorizer (usando a lista de stop words correta)\n",
    "y = np.array(data.sentiment.values)\n",
    "pt_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "# Use o padr√£o padr√£o do CountVectorizer, que √© mais robusto para palavras\n",
    "cv = CountVectorizer(max_features = 1000, stop_words = pt_stopwords) \n",
    "\n",
    "# Use a coluna LIMPA\n",
    "X_final = cv.fit_transform(data.text_pt_cleaned).toarray()\n",
    "\n",
    "print(\"--- Ap√≥s Limpeza dos Espa√ßos ---\")\n",
    "print(\"X_final.shape (Esperado > 77): \", X_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "838f5ace-10f9-4262-997e-6eb086da0c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostras de Textos Limpos Agressivamente (Deve ser 'mais uma vez'):\n",
      "0    maisumavezosrcostnerarrumouumfilmepormuitomais...\n",
      "1    este√©umexemplodomotivopeloqualamaioriadosfilme...\n",
      "2    primeirodetudoeuodeioessesrapsimbecisquen√£opod...\n",
      "Name: text_pt_cleaned_agressive, dtype: object\n",
      "\n",
      "-------------------------------------------\n",
      "üéâ SUCESSO na Vetoriza√ß√£o ap√≥s a Limpeza Agressiva!\n",
      "X_final.shape:  (49459, 1000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Fun√ß√£o de Limpeza Agressiva para garantir que as palavras se juntem\n",
    "def clean_and_reassemble_words(text):\n",
    "    # 1. Remove TODOS os espa√ßos (transforma \"m a i s\" em \"mais\")\n",
    "    text = text.replace(' ', '')\n",
    "    # 2. Substitui m√∫ltiplas ocorr√™ncias de caracteres alfab√©ticos por um espa√ßo\n",
    "    # (Pode ser necess√°rio um pr√©-processamento mais complexo se houver pontua√ß√£o)\n",
    "    # Mas vamos usar o m√©todo mais direto para o seu formato:\n",
    "    \n",
    "    # 3. Tentativa mais simples: Se o texto for apenas letras separadas por espa√ßos,\n",
    "    # remova todos os espa√ßos e adicione UM espa√ßo onde existiam as palavras originais (se soubermos o delimitador)\n",
    "    \n",
    "    # Vamos reverter para a solu√ß√£o original, mas ajustando-a: \n",
    "    # Tenta juntar letras que estavam separadas por um √∫nico espa√ßo\n",
    "    \n",
    "    # Substitui 1 ou mais espa√ßos por um √∫nico espa√ßo. Esta √© a maneira mais robusta:\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Se isso n√£o funcionar, o texto pode ter sido originalmente 'm a i s' e n√£o 'm   a   i   s'.\n",
    "    # A alternativa mais bruta √© for√ßar a jun√ß√£o de todas as letras:\n",
    "    # return re.sub(r'(\\w)\\s(\\w)', r'\\1\\2', text).strip() # Esta √© complexa.\n",
    "    \n",
    "    # **FOCANDO NO PROBLEMA**: Se 'm a i s' √© o problema, TUDO √© token de 1 letra.\n",
    "    # O seu c√≥digo de limpeza deve ser o que apresentei na √∫ltima resposta:\n",
    "    # text = re.sub(r'\\s{2,}', ' ', text) # Substitui 2+ espa√ßos por 1 (deve lidar com o 'm a i s')\n",
    "    \n",
    "    # Se o erro persiste, vamos assumir que a limpeza n√£o est√° juntando 'm a i s'. \n",
    "    # Vamos tentar uma limpeza que remove todos os espa√ßos, exceto os que delimitavam as palavras.\n",
    "    # Baseado na sua amostra: 'm a i s   u m a' => o delimitador de palavras era 3 espa√ßos ou mais.\n",
    "    # Vamos usar uma REGRA SIMPLES: se h√° 3+ espa√ßos, √© um delimitador de palavras.\n",
    "    \n",
    "    text = re.sub(r'\\s{3,}', ' [DELIMITER] ', text) # Marca o delimitador de palavras\n",
    "    text = text.replace(' ', '') # Remove os espa√ßos entre as letras\n",
    "    text = text.replace('[DELIMITER]', ' ') # Restaura o espa√ßo de palavras\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# 1. Aplicar a nova limpeza agressiva\n",
    "data['text_pt_cleaned_agressive'] = data['text_pt'].apply(clean_and_reassemble_words)\n",
    "\n",
    "print(\"Amostras de Textos Limpos Agressivamente (Deve ser 'mais uma vez'):\")\n",
    "print(data.text_pt_cleaned_agressive.head(3))\n",
    "\n",
    "# 2. VETORIZA√á√ÉO COM STOP WORDS (TENTATIVA 1)\n",
    "y = np.array(data.sentiment.values)\n",
    "pt_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "cv = CountVectorizer(max_features = 1000, stop_words = pt_stopwords) \n",
    "\n",
    "try:\n",
    "    X_final = cv.fit_transform(data.text_pt_cleaned_agressive).toarray()\n",
    "    print(\"\\n-------------------------------------------\")\n",
    "    print(\"üéâ SUCESSO na Vetoriza√ß√£o ap√≥s a Limpeza Agressiva!\")\n",
    "    print(\"X_final.shape: \", X_final.shape)\n",
    "\n",
    "except ValueError:\n",
    "    # 3. VETORIZA√á√ÉO SEM STOP WORDS (TENTATIVA 2)\n",
    "    cv_sem_stopwords = CountVectorizer(max_features = 1000)\n",
    "    X_final = cv_sem_stopwords.fit_transform(data.text_pt_cleaned_agressive).toarray()\n",
    "    print(\"\\n-------------------------------------------\")\n",
    "    print(\"üéâ SUCESSO na Vetoriza√ß√£o (SEM Stop Words)!\")\n",
    "    print(\"X_final.shape: \", X_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a63828e-2294-4949-aabc-2970c7a255ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "574b5fe7-5ac8-49bc-a4cb-5e0f2ac0b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'neg' 'neg' ... 'pos' 'pos' 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d014c0e9-32e3-499a-bcd0-4ef526645a52",
   "metadata": {},
   "source": [
    "2. train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "706713e8-8b8a-4681-ac62-9c215c3ec6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : X = (39567, 1000), y = (39567,)\n",
      "Test shapes : X = (9892, 1000), y = (9892,)\n"
     ]
    }
   ],
   "source": [
    "trainx, testx, trainy, testy = train_test_split(X_final,y,test_size=0.2,random_state=9)\n",
    "print(\"Train shape : X = {}, y = {}\".format(trainx.shape,trainy.shape))\n",
    "print(\"Test shapes : X = {}, y = {}\".format(testx.shape, testy.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ad9e7-ad44-4883-9bd4-e2e3c5a6b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Defining the models and training them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "158269df-b356-425c-a501-d5fd0b0969f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>BernoulliNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.BernoulliNB.html\">?<span>Documentation for BernoulliNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>BernoulliNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb,mnb,bnb = GaussianNB(),MultinomialNB(alpha=1.0,fit_prior=True),BernoulliNB(alpha=1.0,fit_prior=True)\n",
    "gnb.fit(trainx,trainy)\n",
    "mnb.fit(trainx,trainy)\n",
    "bnb.fit(trainx,trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c65d5-2c1e-4f4f-a8ba-16138e1209e7",
   "metadata": {},
   "source": [
    "4. Prediction and accuracy metrics to choose best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cbb15bc-ecbc-4ba6-8c41-a62e664ccf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypg = gnb.predict(testx)\n",
    "ypm = mnb.predict(testx)\n",
    "ypb = bnb.predict(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a130474-2837-486e-8719-c61f49093b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian =  0.5098059037606146\n",
      "Multinomial =  0.503538212697129\n",
      "Bernoulli =  0.5098059037606146\n"
     ]
    }
   ],
   "source": [
    "print(\"Gaussian = \",accuracy_score(testy,ypg))\n",
    "print(\"Multinomial = \",accuracy_score(testy,ypm))\n",
    "print(\"Bernoulli = \",accuracy_score(testy,ypb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42365d86-c141-44fb-bab1-a1382e9d9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bnb,open('modell.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b8d82-003b-4dc7-ac2a-0933d863b6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
